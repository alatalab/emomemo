{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import expanduser, isfile\n",
    "\n",
    "from gensim import models\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re,math\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Flatten,Reshape,Dropout,Dense,Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine import Input\n",
    "from keras.engine.topology import Merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "emotion_map={\"anger\":\"😾\", \"enjoyment\":\"😺\", \"fear\":\"🙀\", \"disgust\":\"😼\", \"sadness\":\"😿\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word2vec_file='word2vec'\n",
    "tokenizer_file='tokenizer.pickle'\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "def stem(word):return stemmer.stem(word)\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    if sentence is None:\n",
    "        return None\n",
    "    url_regex =r\"(\\bhttps?://)?[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]\"\n",
    "    sentence=re.sub(url_regex , \"\" , sentence)\n",
    "    sentence=sentence.replace(\"?\" , \" questionmark \")\n",
    "    sentence=sentence.replace(\"!\" , \" exclamationmark \")\n",
    "    sentence=re.sub(r\"\\){2,}\" , \" megasmilemoji \",sentence)\n",
    "    sentence=sentence.replace(\")\" , \" smilemoji \")\n",
    "    sentence=sentence.lower()\n",
    "    words = re.split(\"[\\W\\d_\\\"']\" , sentence)\n",
    "    words = filter(lambda item: 10>len(item)>1, words)\n",
    "    return [stem(word) for word in words]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not isfile(tokenizer_file):\n",
    "    file_path = expanduser(\"~\")+\"/Downloads/facebook-chroneus/html/messages.htm\" \n",
    "    soup = BeautifulSoup(open(file_path),'html.parser')\n",
    "    sentences=[]\n",
    "    names =[\"Сергей Левченко\",\"1145575406@facebook.com\"]\n",
    "    with open('text.csv', 'w') as messages:\n",
    "        for val in reversed(soup.select('div.message_header span.user')):\n",
    "            text=val.parent.parent.next_sibling.text.replace('\\n', ' ')\n",
    "            if len(text)>1:\n",
    "                text=bytes(text, 'utf-8').decode('utf-8','ignore')\n",
    "                sentences.append(\" \".join(split_sentence(text)))\n",
    "                if val.text in names:\n",
    "                    date=dateparser.parse(val.next_sibling.text.split(\"UTC\")[0])\n",
    "                    messages.write ('\"'+text+'\",'+str(date)+',\\n')\n",
    "\n",
    "    w2v_model = models.Word2Vec([sentence.split() for sentence in sentences],\n",
    "                         min_count=1, workers=4, size=200, iter=10, sg=1, window=10)\n",
    "    w2v_model.save(word2vec_file)\n",
    "    w2v_model.vocab\n",
    "    print (w2v_model.most_similar(positive=[stem('счастье')], negative=[stem('жопа')]))\n",
    "   \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    with open(tokenizer_file, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "marked_file = open('text_marked.csv','w')\n",
    "marked_file.write(\"text,date,emotions\\n\")\n",
    "with open('text.csv', 'r') as messages:\n",
    "    for line in messages:\n",
    "        print(line.split(\",\")[0])\n",
    "        question=\"what emotion are you feeling about it?(\"+\" \".join(emotion_map.keys())+\")\"\n",
    "        emotion = input(question).strip(\"\\n\")\n",
    "        marked_file.write(line.strip(\"\\n\")+\"\\\"\"+emotion+\"\\\"\\n\")\n",
    "        time.sleep(1)\n",
    "        clear_output()\n",
    "marked_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('text_marked.csv').dropna()\n",
    "\n",
    "texts=df.text.apply(lambda x: \" \".join(split_sentence(x)))   \n",
    "\n",
    "def prepare_text(texts):\n",
    "    with open(tokenizer_file, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    text_seq=[]\n",
    "    for text in texts:\n",
    "        text_seq.append(\" \".join(split_sentence(text)))\n",
    "    sqs = tokenizer.texts_to_sequences(text_seq)\n",
    "    \n",
    "    return sequence.pad_sequences(sqs, maxlen=32)\n",
    "                                    \n",
    "X=prepare_text(texts)\n",
    "\n",
    "\n",
    "for emotion in emotion_map.keys():\n",
    "    df[emotion]=df.apply(lambda x: 1 if emotion in x[\"emotions\"].lower() else 0, axis=1)\n",
    "    \n",
    "Y=df.as_matrix(emotion_map.keys())\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_cnn( max_length=32, embedding_size=200, ngram_filters=[1,2,3,7], n_feature_maps=32, dropout=0.5, n_hidden=16):\n",
    "  \n",
    "\n",
    "    convs = []\n",
    "    functional_input = Input(shape=(1,max_length, embedding_size))\n",
    "    \n",
    "    for n_gram in ngram_filters:\n",
    "        conv = Convolution2D(nb_filter=n_feature_maps, nb_row=n_gram, nb_col=embedding_size,\n",
    "                             activation='relu', dim_ordering='th')(functional_input)\n",
    "        pool = MaxPooling2D(pool_size=(max_length - n_gram + 1, 1), dim_ordering='th')(conv)\n",
    "        flatten=Flatten()(pool)\n",
    "        convs.append(flatten)\n",
    "    merged=Merge(mode='concat')(convs)\n",
    "    functionalModel = Model(input=functional_input, output=merged)\n",
    "\n",
    "    w2v_model=models.Word2Vec.load(word2vec_file)\n",
    "\n",
    "    weights = w2v_model.syn0\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=weights.shape[0],input_length=max_length,\n",
    "                                output_dim=weights.shape[1], weights=[weights], \n",
    "                                name='embedding')    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Reshape((1, max_length, embedding_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(functionalModel)\n",
    "    model.add(Dense(n_hidden))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(len(emotion_map.keys()),))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model=ngram_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "callbacks_list=[ModelCheckpoint(\"checkpoints\"),TensorBoard()]\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=64,\n",
    "          nb_epoch=42,\n",
    "          validation_split=0.2, show_accuracy=True, shuffle=True,\n",
    "          verbose=0,callbacks=callbacks_list) \n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n",
    "def reply(message):\n",
    "    messages=[]\n",
    "    messages.append(message)\n",
    "    marks=model.predict(prepare_text(messages))\n",
    "    replies=\"\"\n",
    "    for (emotion,value) in zip(emotion_map.keys(),marks[0].tolist()):\n",
    "        if value>0.6 :\n",
    "            replies=replies+emotion_map.get(emotion) +' '\n",
    "    return (replies,marks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reply(\"hi\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flask import Flask, request\n",
    "from fbmq import Page\n",
    "import socket,os \n",
    "\n",
    "\n",
    "DEBUG=False \n",
    "\n",
    "if DEBUG:\n",
    "    hostname=socket.getfqdn()\n",
    "    from werkzeug.serving import make_ssl_devcert\n",
    "\n",
    "    if not os.path.exists(hostname+\".crt\"):\n",
    "        make_ssl_devcert(hostname, host=hostname)\n",
    "else:\n",
    "    hostname=\"localhost\"\n",
    "\n",
    "CONFIG = {\n",
    "        'VERIFY_TOKEN': 'verify',\n",
    "        'SERVER_URL': 'https://',\n",
    "        'PAGE_KEY':\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "page = Page(CONFIG['PAGE_KEY'])\n",
    "\n",
    "\n",
    "@app.route('/webhook', methods=['GET'])\n",
    "def validate():\n",
    "        print (request.args)\n",
    "        if request.args.get('hub.mode', '') == 'subscribe' and \\\n",
    "                                        request.args.get('hub.verify_token', '') == CONFIG['VERIFY_TOKEN']:\n",
    "\n",
    "                print(\"Validating webhook\")\n",
    "\n",
    "                return request.args.get('hub.challenge', '')\n",
    "        else:\n",
    "                return 'Failed validation. Make sure the validation tokens match.'\n",
    "\n",
    "@app.route('/webhook', methods=['POST'])\n",
    "def webhook():\n",
    "    payload = request.get_data(as_text=True)\n",
    "    print(payload)\n",
    "    page.handle_webhook(payload)\n",
    "    return \"ok\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def hi():\n",
    "    if(request.args.get('message', None)):\n",
    "        return \" \\n\".join(str(reply(request.args.get('message', None))))\n",
    "    return \"I am ready\"\n",
    "\n",
    "@page.handle_message\n",
    "def message_handler(event):\n",
    "    sender_id = event.sender_id\n",
    "    message = event.message_text\n",
    "    print(event)\n",
    "    if (event.message.get(\"sticker_id\", None)):\n",
    "        page.send(sender_id, \"Dont understand stickers\")\n",
    "    if(message):\n",
    "        page.send(sender_id,reply(message)[0])\n",
    "\n",
    "@page.after_send\n",
    "def after_send(payload, response):\n",
    "    print(\"complete\")\n",
    "    \n",
    "\n",
    "\n",
    "context = (hostname+'.crt', hostname+'.key')\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False,host=\"0.0.0.0\",ssl_context=context,port=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
