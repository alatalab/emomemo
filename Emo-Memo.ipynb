{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import expanduser, isfile\n",
    "\n",
    "from gensim import models\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re,math\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Flatten,Reshape,Dropout,Dense,Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine import Input\n",
    "from keras.engine.topology import Merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "emotion_map={\"anger\":\"ðŸ˜¾\", \"enjoyment\":\"ðŸ˜º\", \"fear\":\"ðŸ™€\", \"disgust\":\"ðŸ˜¼\", \"sadness\":\"ðŸ˜¿\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word2vec_file='word2vec'\n",
    "tokenizer_file='tokenizer.pickle'\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "def stem(word):return stemmer.stem(word)\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    if sentence is None:\n",
    "        return None\n",
    "    url_regex =r\"(\\bhttps?://)?[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]\"\n",
    "    sentence=re.sub(url_regex , \"\" , sentence)\n",
    "    sentence=sentence.replace(\"?\" , \" questionmark \")\n",
    "    sentence=sentence.replace(\"!\" , \" exclamationmark \")\n",
    "    sentence=re.sub(r\"\\){2,}\" , \" megasmilemoji \",sentence)\n",
    "    sentence=sentence.replace(\")\" , \" smilemoji \")\n",
    "    sentence=sentence.lower()\n",
    "    words = re.split(\"[\\W\\d_\\\"']\" , sentence)\n",
    "    words = filter(lambda item: 10>len(item)>1, words)\n",
    "    return [stem(word) for word in words]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not isfile(tokenizer_file):\n",
    "    file_path = expanduser(\"~\")+\"/Downloads/facebook-chroneus/html/messages.htm\" \n",
    "    soup = BeautifulSoup(open(file_path),'html.parser')\n",
    "    sentences=[]\n",
    "    names =[\"Ð¡ÐµÑ€Ð³ÐµÐ¹ Ð›ÐµÐ²Ñ‡ÐµÐ½ÐºÐ¾\",\"1145575406@facebook.com\"]\n",
    "    with open('text.csv', 'w') as messages:\n",
    "        for val in reversed(soup.select('div.message_header span.user')):\n",
    "            text=val.parent.parent.next_sibling.text.replace('\\n', ' ')\n",
    "            if len(text)>1:\n",
    "                text=bytes(text, 'utf-8').decode('utf-8','ignore')\n",
    "                sentences.append(\" \".join(split_sentence(text)))\n",
    "                if val.text in names:\n",
    "                    date=dateparser.parse(val.next_sibling.text.split(\"UTC\")[0])\n",
    "                    messages.write ('\"'+text+'\",'+str(date)+',\\n')\n",
    "\n",
    "    w2v_model = models.Word2Vec([sentence.split() for sentence in sentences],\n",
    "                         min_count=1, workers=4, size=200, iter=10, sg=1, window=10)\n",
    "    w2v_model.save(word2vec_file)\n",
    "    w2v_model.vocab\n",
    "    print (w2v_model.most_similar(positive=[stem('ÑÑ‡Ð°ÑÑ‚ÑŒÐµ')], negative=[stem('Ð¶Ð¾Ð¿Ð°')]))\n",
    "   \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    with open(tokenizer_file, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ñ…Ð°Ñ…Ð° Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ñ‚\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e42668176570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"what emotion are you feeling about it?(\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmarked_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "marked_file = open('text_marked.csv','w')\n",
    "marked_file.write(\"text,date,emotions\\n\")\n",
    "with open('text.csv', 'r') as messages:\n",
    "    for line in messages:\n",
    "        print(line.split(\",\")[0])\n",
    "        question=\"what emotion are you feeling about it?(\"+\" \".join(emotion_map.keys())+\")\"\n",
    "        emotion = input(question).strip(\"\\n\")\n",
    "        marked_file.write(line.strip(\"\\n\")+\"\\\"\"+emotion+\"\\\"\\n\")\n",
    "        time.sleep(1)\n",
    "        clear_output()\n",
    "marked_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0dbf1e1d2755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_marked.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('text_marked.csv').dropna()\n",
    "\n",
    "texts=df.text.apply(lambda x: \" \".join(split_sentence(x)))   \n",
    "\n",
    "def prepare_text(texts):\n",
    "    with open(tokenizer_file, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    text_seq=[]\n",
    "    for text in texts:\n",
    "        text_seq.append(\" \".join(split_sentence(text)))\n",
    "    sqs = tokenizer.texts_to_sequences(text_seq)\n",
    "    \n",
    "    return sequence.pad_sequences(sqs, maxlen=32)\n",
    "                                    \n",
    "X=prepare_text(texts)\n",
    "\n",
    "\n",
    "for emotion in emotion_map.keys():\n",
    "    df[emotion]=df.apply(lambda x: 1 if emotion in x[\"emotions\"].lower() else 0, axis=1)\n",
    "    \n",
    "Y=df.as_matrix(emotion_map.keys())\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_cnn( max_length=32, embedding_size=200, ngram_filters=[1,2,3,7], n_feature_maps=32, dropout=0.5, n_hidden=16):\n",
    "  \n",
    "\n",
    "    convs = []\n",
    "    functional_input = Input(shape=(1,max_length, embedding_size))\n",
    "    \n",
    "    for n_gram in ngram_filters:\n",
    "        conv = Convolution2D(nb_filter=n_feature_maps, nb_row=n_gram, nb_col=embedding_size,\n",
    "                             activation='relu', dim_ordering='th')(functional_input)\n",
    "        pool = MaxPooling2D(pool_size=(max_length - n_gram + 1, 1), dim_ordering='th')(conv)\n",
    "        flatten=Flatten()(pool)\n",
    "        convs.append(flatten)\n",
    "    merged=Merge(mode='concat')(convs)\n",
    "    functionalModel = Model(input=functional_input, output=merged)\n",
    "\n",
    "    w2v_model=models.Word2Vec.load(word2vec_file)\n",
    "\n",
    "    weights = w2v_model.syn0\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=weights.shape[0],input_length=max_length,\n",
    "                                output_dim=weights.shape[1], weights=[weights], \n",
    "                                name='embedding')    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Reshape((1, max_length, embedding_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(functionalModel)\n",
    "    model.add(Dense(n_hidden))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(len(emotion_map.keys()),))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model=ngram_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "callbacks_list=[ModelCheckpoint(\"checkpoints\"),TensorBoard()]\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=64,\n",
    "          nb_epoch=42,\n",
    "          validation_split=0.2, show_accuracy=True, shuffle=True,\n",
    "          verbose=0,callbacks=callbacks_list) \n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n",
    "def reply(message):\n",
    "    messages=[]\n",
    "    messages.append(message)\n",
    "    marks=model.predict(prepare_text(messages))\n",
    "    replies=\"\"\n",
    "    for (emotion,value) in zip(emotion_map.keys(),marks[0].tolist()):\n",
    "        if value>0.6 :\n",
    "            replies=replies+emotion_map.get(emotion) +' '\n",
    "    return (replies,marks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reply(\"hi\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flask import Flask, request\n",
    "from fbmq import Page\n",
    "import socket,os \n",
    "\n",
    "\n",
    "DEBUG=False \n",
    "\n",
    "if DEBUG:\n",
    "    hostname=socket.getfqdn()\n",
    "    from werkzeug.serving import make_ssl_devcert\n",
    "\n",
    "    if not os.path.exists(hostname+\".crt\"):\n",
    "        make_ssl_devcert(hostname, host=hostname)\n",
    "else:\n",
    "    hostname=\"localhost\"\n",
    "\n",
    "CONFIG = {\n",
    "        'VERIFY_TOKEN': 'verify',\n",
    "        'SERVER_URL': 'https://',\n",
    "        'PAGE_KEY':\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "page = Page(CONFIG['PAGE_KEY'])\n",
    "\n",
    "\n",
    "@app.route('/webhook', methods=['GET'])\n",
    "def validate():\n",
    "        print (request.args)\n",
    "        if request.args.get('hub.mode', '') == 'subscribe' and \\\n",
    "                                        request.args.get('hub.verify_token', '') == CONFIG['VERIFY_TOKEN']:\n",
    "\n",
    "                print(\"Validating webhook\")\n",
    "\n",
    "                return request.args.get('hub.challenge', '')\n",
    "        else:\n",
    "                return 'Failed validation. Make sure the validation tokens match.'\n",
    "\n",
    "@app.route('/webhook', methods=['POST'])\n",
    "def webhook():\n",
    "    payload = request.get_data(as_text=True)\n",
    "    print(payload)\n",
    "    page.handle_webhook(payload)\n",
    "    return \"ok\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def hi():\n",
    "    if(request.args.get('message', None)):\n",
    "        return \" \\n\".join(str(reply(request.args.get('message', None))))\n",
    "    return \"I am ready\"\n",
    "\n",
    "@page.handle_message\n",
    "def message_handler(event):\n",
    "    sender_id = event.sender_id\n",
    "    message = event.message_text\n",
    "    print(event)\n",
    "    if (event.message.get(\"sticker_id\", None)):\n",
    "        page.send(sender_id, \"Dont understand stickers\")\n",
    "    if(message):\n",
    "        page.send(sender_id,reply(message)[0])\n",
    "\n",
    "@page.after_send\n",
    "def after_send(payload, response):\n",
    "    print(\"complete\")\n",
    "    \n",
    "\n",
    "\n",
    "context = (hostname+'.crt', hostname+'.key')\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False,host=\"0.0.0.0\",ssl_context=context,port=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
